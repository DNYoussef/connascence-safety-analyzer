============================= test session starts =============================
platform win32 -- Python 3.12.5, pytest-9.0.1, pluggy-1.5.0 -- C:\Python312\python.exe
cachedir: .pytest_cache
hypothesis profile 'default'
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
metadata: {'Python': '3.12.5', 'Platform': 'Windows-10-10.0.19045-SP0', 'Packages': {'pytest': '9.0.1', 'pluggy': '1.5.0'}, 'Plugins': {'anyio': '4.11.0', 'Faker': '18.13.0', 'hypothesis': '6.138.2', 'langsmith': '0.4.21', 'asyncio': '1.3.0', 'bandit': '0.6.1', 'benchmark': '4.0.0', 'cov': '4.1.0', 'env': '1.1.5', 'flask': '1.3.0', 'json-report': '1.5.0', 'metadata': '3.1.1', 'mock': '3.14.0', 'randomly': '3.16.0', 'timeout': '2.3.1', 'xdist': '3.6.1'}, 'JAVA_HOME': 'C:\\Program Files\\AdoptOpenJDK\\jre-8.0.265.01-hotspot\\'}
Using --randomly-seed=104340673
rootdir: C:\Users\17175\Desktop\connascence
configfile: pyproject.toml
testpaths: tests, fixtures
plugins: anyio-4.11.0, Faker-18.13.0, hypothesis-6.138.2, langsmith-0.4.21, asyncio-1.3.0, bandit-0.6.1, benchmark-4.0.0, cov-4.1.0, env-1.1.5, flask-1.3.0, json-report-1.5.0, metadata-3.1.1, mock-3.14.0, randomly-3.16.0, timeout-2.3.1, xdist-3.6.1
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 964 items / 4 skipped

tests/test_performance_regression.py::TestPerformanceRegression::test_medium_file_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_analysis_caching_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_memory_scalability FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_concurrent_analysis_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_directory_analysis_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_memory_cleanup_after_analysis FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_large_file_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_small_file_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_large_violation_count_performance FAILED [  0%]
tests/test_performance_regression.py::TestPerformanceRegression::test_performance_constants_compliance FAILED [  1%]datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).

Run started:2025-11-15 14:16:29.141309

Test results:
	No issues identified.

Code scanned:
	Total lines of code: 0
	Total lines skipped (#nosec): 0

Run metrics:
	Total issues (by severity):
		Undefined: 0
		Low: 0
		Medium: 0
		High: 0
	Total issues (by confidence):
		Undefined: 0
		Low: 0
		Medium: 0
		High: 0
Files skipped (0):


================================== FAILURES ===================================
___________ TestPerformanceRegression.test_medium_file_performance ____________
tests\test_performance_regression.py:130: in test_medium_file_performance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
_________ TestPerformanceRegression.test_analysis_caching_performance _________
tests\test_performance_regression.py:302: in test_analysis_caching_performance
    with PerformanceMonitor() as cold_monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
______________ TestPerformanceRegression.test_memory_scalability ______________
tests\test_performance_regression.py:202: in test_memory_scalability
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
_______ TestPerformanceRegression.test_concurrent_analysis_performance ________
tests\test_performance_regression.py:257: in test_concurrent_analysis_performance
    with PerformanceMonitor() as sequential_monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
________ TestPerformanceRegression.test_directory_analysis_performance ________
tests\test_performance_regression.py:170: in test_directory_analysis_performance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
________ TestPerformanceRegression.test_memory_cleanup_after_analysis _________
tests\test_performance_regression.py:279: in test_memory_cleanup_after_analysis
    initial_memory = psutil.Process(os.getpid()).memory_info().rss
                                    ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
____________ TestPerformanceRegression.test_large_file_performance ____________
tests\test_performance_regression.py:149: in test_large_file_performance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
____________ TestPerformanceRegression.test_small_file_performance ____________
tests\test_performance_regression.py:108: in test_small_file_performance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
______ TestPerformanceRegression.test_large_violation_count_performance _______
tests\test_performance_regression.py:369: in test_large_violation_count_performance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active
_______ TestPerformanceRegression.test_performance_constants_compliance _______
tests\test_performance_regression.py:234: in test_performance_constants_compliance
    with PerformanceMonitor() as monitor:
         ^^^^^^^^^^^^^^^^^^^^
tests\test_performance_regression.py:33: in __init__
    self.process = psutil.Process(os.getpid())
                                  ^^
E   NameError: name 'os' is not defined. Did you forget to import 'os'
----------------------------- Captured log setup ------------------------------
WARNING  analyzer.optimization.memory_monitor:memory_monitor.py:257 Memory monitoring already active

---------- coverage: platform win32, python 3.12.5-final-0 -----------
Name                                     Stmts   Miss Branch BrPart   Cover   Missing
-------------------------------------------------------------------------------------
autofix\__init__.py                          0      0      0      0 100.00%
autofix\class_splits.py                    206    206     80      0   0.00%   21-396
autofix\core.py                            243    243     78      0   0.00%   14-477
autofix\god_objects.py                       5      5      0      0   0.00%   15-22
autofix\magic_literals.py                  151    116     70      0  15.84%   46-49, 60-73, 89-109, 119-149, 153-168, 172-193, 198-209, 213-216, 220-231, 238-239, 242-244, 247-249, 252-254, 261-262, 266-268, 272-281, 285-287, 291, 295-298
autofix\param_bombs.py                     182    145     68      0  14.80%   56-57, 63-76, 90-112, 123-134, 138-161, 166-177, 181-194, 198-207, 211-218, 223-246, 251-275, 279-294, 298-305, 312-315, 318-320, 323-329
autofix\patch_api.py                        44     13      8      0  59.62%   31-32, 46-51, 61, 68, 71, 78, 81
autofix\patch_generator.py                 230    230     76      0   0.00%   27-608
autofix\tier_classifier.py                  95     95     38      0   0.00%   32-369
autofix\type_hints.py                      192    159     88      0  11.79%   45-62, 75-88, 102-110, 120-146, 152-193, 199-237, 241-260, 264-267, 271-297, 301, 305, 312-313, 316-318, 325-328, 331-334, 338-342, 346-351, 354-356
cli\__init__.py                             11      2      2      1  76.92%   20-21, 25->exit
cli\__main__.py                             18     18      0      0   0.00%   3-27
cli\connascence.py                          70     44     10      1  33.75%   19-22, 28, 32-73, 78, 81-82, 87, 90, 95, 106-114, 120-125, 130, 133, 143->exit
mcp\__init__.py                              2      0      0      0 100.00%
mcp\cli.py                                 126    126     22      0   0.00%   14-254
mcp\enhanced_server.py                     340    340     80      0   0.00%   18-761
mcp\server.py                              325    273     94      0  12.41%   41-88, 93-124, 130-132, 143-144, 147-150, 155-159, 166-188, 193-259, 263-319, 323, 328-345, 349-367, 371-399, 411-440, 444-481, 485-497, 501, 514-599, 615, 619, 625-627, 634-646, 651-696, 707-727, 736, 745-768, 780, 784, 805-809, 813-820, 824-829, 833-841, 845, 850-854
policy\__init__.py                           5      0      0      0 100.00%
policy\baselines.py                        255    185     48      0  23.10%   51, 64, 89, 101, 122-154, 170-179, 190-217, 221-222, 226-235, 241-275, 297-324, 328-336, 340-348, 352-357, 361-365, 369, 380-384, 400-404, 412-413, 420-439, 443, 456, 462-485, 501-502, 506-521, 526-538, 543-547, 552-584, 590-595, 599
policy\budgets.py                          179    132     56      0  20.00%   75-83, 87-106, 111-118, 124-175, 189-206, 210-217, 221-250, 254-279, 286-289, 293, 297-303, 309-341, 351-359, 371-382, 386-395, 399, 403-405, 410-411, 416-419
policy\drift.py                            196    196     42      0   0.00%   30-459
policy\manager.py                          222    175    104      0  14.42%   25-27, 69-135, 140-150, 155-172, 180-183, 187, 191-217, 221-239, 243-250, 254-278, 282-286, 292-351, 356-384, 389-397, 407-426, 430-467, 480-486, 490
policy\presets\__init__.py                   0      0      0      0 100.00%
policy\presets\general_safety_rules.py       3      3      0      0   0.00%   15-18
policy\waivers.py                          239    175     66      0  20.98%   42-44, 95-101, 105-124, 131-140, 144-173, 177-198, 202-228, 242-265, 269-282, 286-299, 303-306, 310-313, 317, 325-334, 338-352, 356-379, 389-416, 420-424, 438-440, 447-448, 451-453
-------------------------------------------------------------------------------------
TOTAL                                     3339   2881   1030      2  10.53%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_medium_file_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_analysis_caching_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_memory_scalability
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_concurrent_analysis_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_directory_analysis_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_memory_cleanup_after_analysis
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_large_file_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_small_file_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_large_violation_count_performance
FAILED tests/test_performance_regression.py::TestPerformanceRegression::test_performance_constants_compliance
!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 10 failures !!!!!!!!!!!!!!!!!!!!!!!!!!
======================= 10 failed, 4 skipped in 12.43s ========================
